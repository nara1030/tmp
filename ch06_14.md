14강. 사이트 전체의 특징/경향 찾기
=====
## 목차
1. [날짜별 방문자 수와 방문횟수 그리고 페이지 뷰 집계하기](#날짜별-방문자-수와-방문횟수-그리고-페이지-뷰-집계하기)
2. [페이지별 쿠키와 방문횟수 그리고 페이지 뷰 집계하기](#페이지별-쿠키와-방문횟수-그리고-페이지-뷰-집계하기)
3. [유입원별로 방문횟수 또는 CVR 집계하기](#유입원별로-방문횟수-또는-CVR-집계하기)
4. [접근 요일 및 시간대 파악하기](#접근-요일-및-시간대-파악하기)
5. [리마인드](#리마인드)

## 날짜별 방문자 수와 방문횟수 그리고 페이지 뷰 집계하기
```sql
DROP TABLE IF EXISTS access_log;
CREATE TABLE access_log(
    stamp         varchar(255)
  , short_session varchar(255)
  , long_session  varchar(255)
  , url           text
  , referrer      text
);

INSERT INTO access_log
VALUES
    ('2016-10-01 12:00:00', '0CVKaz', '1CwlSX', 'http://www.example.com/?utm_source=google&utm_medium=search'       , 'http://www.google.co.jp/xxx'      )
  , ('2016-10-01 13:00:00', '0CVKaz', '1CwlSX', 'http://www.example.com/detail?id=1'                                , ''                                 )
  , ('2016-10-01 13:00:00', '1QceiB', '3JMO2k', 'http://www.example.com/list/cd'                                    , ''                                 )
  , ('2016-10-01 14:00:00', '1QceiB', '3JMO2k', 'http://www.example.com/detail?id=1'                                , 'http://search.google.co.jp/xxx'   )
  , ('2016-10-01 15:00:00', '1hI43A', '6SN6DD', 'http://www.example.com/list/newly'                                 , ''                                 )
  , ('2016-10-01 16:00:00', '1hI43A', '6SN6DD', 'http://www.example.com/list/cd'                                    , 'http://www.example.com/list/newly')
  , ('2016-10-01 17:00:00', '2bGs3i', '1CwlSX', 'http://www.example.com/'                                           , ''                                 )
  , ('2016-10-01 18:00:00', '2is8PX', '7Dn99b', 'http://www.example.com/detail?id=2'                                , 'https://twitter.com/xxx'          )
  , ('2016-10-02 12:00:00', '2mmGwD', 'EFnoNR', 'http://www.example.com/'                                           , ''                                 )
  , ('2016-10-02 13:00:00', '2mmGwD', 'EFnoNR', 'http://www.example.com/list/cd'                                    , 'http://search.google.co.jp/xxx'   )
  , ('2016-10-02 14:00:00', '3CEHe1', 'FGkTe9', 'http://www.example.com/list/dvd'                                   , ''                                 )
  , ('2016-10-02 15:00:00', '3Gv8vO', '1CwlSX', 'http://www.example.com/detail?id=2'                                , ''                                 )
  , ('2016-10-02 16:00:00', '3cv4gm', 'KBlKgT', 'http://www.example.com/list/newly'                                 , 'http://search.yahoo.co.jp/xxx'    )
  , ('2016-10-02 17:00:00', '3cv4gm', 'KBlKgT', 'http://www.example.com/'                                           , 'https://www.facebook.com/xxx'     )
  , ('2016-10-02 18:00:00', '690mvB', 'FGkTe9', 'http://www.example.com/list/dvd?utm_source=yahoo&utm_medium=search', 'http://www.yahoo.co.jp/xxx'       )
  , ('2016-10-03 12:00:00', '6oABhM', '3JMO2k', 'http://www.example.com/detail?id=3'                                , 'http://search.yahoo.co.jp/xxx'    )
  , ('2016-10-03 13:00:00', '7jjxQX', 'KKTw9P', 'http://www.example.com/?utm_source=mynavi&utm_medium=affiliate'    , 'http://www.mynavi.jp/xxx'         )
  , ('2016-10-03 14:00:00', 'AAuoEU', '6SN6DD', 'http://www.example.com/list/dvd'                                   , 'https://www.facebook.com/xxx'     )
  , ('2016-10-03 15:00:00', 'AAuoEU', '6SN6DD', 'http://www.example.com/list/newly'                                 , ''                                 )
;

DROP TABLE IF EXISTS purchase_log;
CREATE TABLE purchase_log(
    stamp         varchar(255)
  , short_session varchar(255)
  , long_session  varchar(255)
  , purchase_id   integer
  , amount        integer
);

INSERT INTO purchase_log
VALUES
    ('2016-10-01 15:00:00', '0CVKaz', '1CwlSX', 1, 1000)
  , ('2016-10-01 16:00:00', '2is8PX', '7Dn99b', 2, 1000)
  , ('2016-10-01 20:00:00', '2is8PX', '7Dn99b', 3, 1000)
  , ('2016-10-02 14:00:00', '2is8PX', '7Dn99b', 4, 1000)
;

-- 1.
SELECT SUBSTRING(stamp, 1, 10) AS dt
	 , COUNT(DISTINCT long_session) AS access_users
	 , COUNT(DISTINCT short_session) AS access_count
	 , COUNT(*) AS page_view
  FROM access_log
 GROUP BY dt
 ORDER BY dt;
```


##### [목차로 이동](#목차)

## 페이지별 쿠키와 방문횟수 그리고 페이지 뷰 집계하기
```sql
-- 2.
-- 2-1. URL별로 집계하기
SELECT url
     , COUNT(DISTINCT short_session) AS access_count
     , COUNT(DISTINCT long_session) AS access_users
     , COUNT(*) AS page_view
  FROM access_log
 GROUP BY url;

-- 2-2. 경로별로 집계하기
WITH access_log_with_path AS (
    -- URL에서 경로 추출하기
    SELECT *
         , SUBSTRING(url from '//[^/]+([^?#]+)') as url_path
      FROM access_log
)
SELECT url_path
     , COUNT(DISTINCT short_session) AS access_count
     , COUNT(DISTINCT long_session) AS access_users
     , COUNT(*) AS page_view
  FROM access_log_with_path
 GROUP BY url_path;

-- 2-3. URL에 의미를 부여해서 집계하는 쿼리
WITH access_log_with_path AS (
    -- URL에서 경로 추출하기
    SELECT *
         , SUBSTRING(url from '//[^/]+([^?#]+)') as url_path
      FROM access_log
)
, access_log_with_split_path AS (
    SELECT *
         , SPLIT_PART(url_path, '/', 2) AS path1
         , SPLIT_PART(url_path, '/', 3) AS path2
      FROM access_log_with_path
)
, access_log_with_page_name AS (
    SELECT *
         , CASE WHEN path1 = 'list'
                     THEN CASE WHEN path2 = 'newly'
                                    THEN 'newly_list'
                               ELSE 'category_list'
                          END
                ELSE url_path
           END AS page_name
      FROM access_log_with_split_path
)
SELECT page_name
     , COUNT(DISTINCT short_session) AS access_count
     , COUNT(DISTINCT long_session) AS access_users
     , COUNT(*) AS page_view
  FROM access_log_with_page_name
 GROUP BY page_name
 ORDER BY page_name;
```


##### [목차로 이동](#목차)

## 유입원별로 방문횟수 또는 CVR 집계하기
```sql
-- 3.
-- 3-1.
WITH access_log_with_parse_info AS (
    SELECT *
         , SUBSTRING(url from 'https?://([^/]*)') AS url_domain
         , SUBSTRING(url from 'utm_source=([^&]*)') AS url_utm_source
         , SUBSTRING(url from 'utm_medium=([^&]*)') AS url_utm_medium
         , SUBSTRING(referrer from 'https?://([^/]*)') AS referrer_domain
      FROM access_log
)
, access_log_with_via_info AS (
    SELECT *
         , ROW_NUMBER() OVER(ORDER BY stamp) AS log_id
         , CASE WHEN url_utm_source <> '' AND url_utm_medium <> ''
                    THEN CONCAT(url_utm_source, '-', url_utm_medium)
                WHEN referrer_domain IN ('search.yahoo.co.jp', 'www.google.co.jp')
                    THEN 'search'
                WHEN referrer_domain IN ('twitter.com', 'www.facebook.com')
                    THEN 'social'
                ELSE 'other'
           END AS via
      FROM access_log_with_parse_info
     WHERE COALESCE(referrer_domain, '') NOT IN ('', url_domain)
)
SELECT via
     , COUNT(1) AS access_count
  FROM access_log_with_via_info
 GROUP BY via
 ORDER BY access_count DESC;

-- 3-2.
WITH access_log_with_parse_info AS (
    SELECT *
         , SUBSTRING(url from 'https?://([^/]*)') AS url_domain
         , SUBSTRING(url from 'utm_source=([^&]*)') AS url_utm_source
         , SUBSTRING(url from 'utm_medium=([^&]*)') AS url_utm_medium
         , SUBSTRING(referrer from 'https?://([^/]*)') AS referrer_domain
      FROM access_log
)
, access_log_with_via_info AS (
    SELECT *
         , ROW_NUMBER() OVER(ORDER BY stamp) AS log_id
         , CASE WHEN url_utm_source <> '' AND url_utm_medium <> ''
                    THEN CONCAT(url_utm_source, '-', url_utm_medium)
                WHEN referrer_domain IN ('search.yahoo.co.jp', 'www.google.co.jp')
                    THEN 'search'
                WHEN referrer_domain IN ('twitter.com', 'www.facebook.com')
                    THEN 'social'
                ELSE 'other'
           END AS via
      FROM access_log_with_parse_info
     WHERE COALESCE(referrer_domain, '') NOT IN ('', url_domain)
)
, access_log_with_purchase_amount AS (
    SELECT a.log_id
         , a.via
         , SUM(CASE WHEN p.stamp::date BETWEEN a.stamp::date AND a.stamp::date + '1 day'::interval
                    THEN amount
               END) AS amount
      FROM access_log_with_via_info AS a
      LEFT OUTER JOIN purchase_log AS p
        ON a.long_session = p.long_session
     GROUP BY a.log_id
            , a.via
)
SELECT via
     , COUNT(1) AS via_count
     , COUNT(amount) AS conversions
     , AVG(100.0 * SIGN(COALESCE(amount, 0))) AS cvr
     , SUM(COALESCE(amount, 0)) AS amount
     , AVG(1.0 * COALESCE(amount, 0)) AS avg_amount
  FROM access_log_with_purchase_amount
 GROUP BY via
 ORDER BY cvr DESC;

```


##### [목차로 이동](#목차)

## 접근 요일 및 시간대 파악하기
```txt
WITH access_log_with_dow AS (
    SELECT stamp
         , date_part('dow', stamp::timestamp) AS dow
         , CAST(SUBSTRING(stamp, 12, 2) AS int) * 60 * 60
           + CAST(SUBSTRING(stamp, 15, 2) AS int) * 60
           + CAST(SUBSTRING(stamp, 18, 2) AS int) AS whole_seconds
         , 30 * 60 as interval_seconds
      FROM access_log
)
, access_log_with_floor_seconds AS (
    SELECT stamp
         , dow
         , CAST((FLOOR(whole_seconds / interval_seconds) * interval_seconds) AS int) AS floor_seconds
      FROM access_log_with_dow
)
, access_log_with_index AS (
    SELECT stamp
         , dow
         , LPAD(FLOOR(floor_seconds / (60 * 60))::text, 2, '0') || ':'
           || LPAD(FLOOR(floor_seconds % (60 * 60) / 60)::text, 2, '0') || ':'
           || LPAD(FLOOR(floor_seconds % 60)::text, 2, '0') AS index_time
      FROM access_log_with_floor_seconds
)
SELECT index_time
     , COUNT(CASE dow WHEN 0 THEN 1 END) sun
     , COUNT(CASE dow WHEN 1 THEN 1 END) mon
     , COUNT(CASE dow WHEN 2 THEN 1 END) tue
     , COUNT(CASE dow WHEN 3 THEN 1 END) wed
     , COUNT(CASE dow WHEN 4 THEN 1 END) thu
     , COUNT(CASE dow WHEN 5 THEN 1 END) fri
     , COUNT(CASE dow WHEN 6 THEN 1 END) sat
  FROM access_log_with_index
 GROUP BY index_time
 ORDER BY index_time;

```


##### [목차로 이동](#목차)

## 리마인드
데이터의 종류는 다음과 같이 나뉜다.

1. 업무 데이터: 대부분 갱신형 데이터
	1. 트랜잭션 데이터: 이를 기반으로 리포트를 만듦
	2. 마스터 데이터: 트랜잭션 데이터를 설명
2. 로그 데이터: 누적형 데이터

업무 데이터 및 로그 데이터의 상세한 특징은 아래와 같다.

1. 업무 데이터
	1. 데이터의 정밀도가 높음(∵ 데이터 정합성 보증)
	2. 데이터 추출 시점에 추출되는 데이터 바뀔 수 있음(∵ 갱신형 데이터)
	3. 다루는 테이블 수가 많음(∵ RDB)
2. 로그 데이터
	1. 시간, 사용자 엔드 포인트, IP, 레퍼러, Cookie 등의 정보 저장
	2. 추출 방법에 따라 데이터 정밀도 달라짐
	3. 과거 데이터 불변(∵ 누적형 데이터)

한편 로그 데이터의 전송 및 축적 방법은 크게 두 가지로 구분된다.

1. 태그, SDK를 통해 사용자 장치에서 데이터를 전송하고 출력하기
2. 서버에서 데이터를 추출하고 출력하기

- - -
* COUNT 함수
	* https://ggmouse.tistory.com/156
	* https://tawool.tistory.com/152
* Postgre function
	* [SUBSTRING](https://w3resource.com/PostgreSQL/substring-function.php)
	* [SPLIT_PART](https://w3resource.com/PostgreSQL/split_part-function.php)
* .

##### [목차로 이동](#목차)
